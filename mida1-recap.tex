\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{inputenc}
\usepackage[a4paper, total={150mm,250mm}]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{subcaption}
\usepackage{fancybox, graphicx}
\usepackage{tikz}
\usepackage{array}
\usepackage{ulem}
\usepackage{enumitem}
\usetikzlibrary{shadows}
\usepackage{listings}
\usepackage{lmodern,textcomp}
%\usepackage[english]{babel}

\newcommand{\question}[5]{
{#1}
\begin{enumerate}[label=\textbf{\alph*)}]
	\item {#2}
	\item {#3}
	\item {#4}
	\item {#5}
\end{enumerate}
}

\usepackage{tikz}
\hypersetup{
    colorlinks=true, %set true if you want colored links
	linkcolor=black,
    linktoc=all,     %set to all if you want both sections and subsections linked 
    urlcolor=blue
}
\title{{\Huge\textbf{MIDA1: Recap}
\\ \LARGE Model Identification and Data Analysis I
\\ \large \textbf{Professor}: Sergio Bittanti  \linebreak
\\ \small \textbf{Authors}: Simone Staffa \linebreak
\\ \small Based on the notes provided by Giulio Alizoni
\\ \small Appendix questions provided by Moreno and @idividebyzero

\vspace{5em}
\begin{figure}[h!]
 \hfill \includegraphics[width=200pt]{images/polimi.png}\hspace*{\fill}
  \label{fig:polimi}
\end{figure}
}
\vspace{1em}
\small{
Released with Beerware License, Rev. 42 (https://spdx.org/licenses/Beerware.html) \linebreak
“As long as you retain this notice you can do whatever you want with this stuff. If we meet some day, and you think this stuff is worth it, you can buy me a beer in return”}
}
\begin{document}
\maketitle
\clearpage
\tableofcontents
\clearpage
\section{Random Variables Refresh}
A random variable $v$ can be described by three main properties:
\begin{itemize}
	\item \textbf{Mean}: $E[v]$. Key property of expected value $E[\alpha_1v_1+\alpha_2v_2] = \alpha_1E[v_1] + \alpha_2E[v_2] $
	\item \textbf{Variance}: $E[(v-E[v])^2] \geq 0$
	\item \textbf{Standard Deviation}: $\sqrt{Var[v]}$
\end{itemize}
\textbf{Random Vector}: a vector composed of random variables.
\\ \\
The \textbf{Covariance} (or Cross Variance) between two elements of it can be defined as:
\center 
$\lambda_{12} = E[(v_1-E[v_1])(v_2-E[v_2])]$ with $V = | v_1 v_2 |^T$
\\
\vspace{1em}
\raggedright
We can define the \textbf{Variance Matrix} as $Var[V]=	\begin{bmatrix}
\lambda_{1,1} & \lambda_{1,2} \\
\lambda_{2,1} & \lambda_{2,2}
\end{bmatrix}$.
This matrix has some properties:
\begin{itemize}
	\item Symmetric
	\item Positive Semi-Definite: $det(Var[v]) \geq 0$
\end{itemize}
Defining the \textbf{covariance coefficient} $\rho=\frac{\lambda_{12}}{\sqrt{\lambda_{11}}*\sqrt{\lambda_{22}}}$, we obtain that $|\rho| \leq 1 $
\section{Random Process Introduction}
A random process (also called \textbf{stochastic process}) is a sequence of random variables. We will focus on \textbf{Stationary Stochastic Process (SSP)} which have the following characteristics:
\begin{itemize}
	\item \textbf{Mean} is constant ($m$)
	\item \textbf{Variance} is constant ($\lambda^2$)
	\item $\gamma(t_1,t_2) = E[(v(t_1)-m)(v(t_2)-m)]$ the \textbf{covariance function}, it depends only on $\tau = t_2 - t_1$ and can therefore be indicated with $\gamma(\tau)$. The variance can be also defined as $\gamma(0) = \lambda^2$
\end{itemize}
A \textbf{White Noise (WN)} is a SSP with $\gamma(0) = 0$ $\forall \tau \neq 0$. A WN is an unpredictable signal meaning that there is NO CORRELATION between $\eta(t_1)$ and $\eta(t_2)$. Usually a WN is defined as $\eta(t) \sim WN(0,\lambda^2)$. Having them with zero-mean is not mandatory.
\section{Familes of SSP}
\subsection{Moving Average Process (MA)}
\center 
$MA(n) : v(t) = c_0 \eta(t) + c_1 \eta(t-1) + ... + c_n \eta(t-n)$
\\
\vspace{1em}
\raggedright
Where $n$ is the order of the process, $\eta(t) \sim WN(0,\lambda^2)$. \\
\begin{itemize}
	\item By definition, \textbf{MA processes are stationary} because they are the result of the sum of stationary processes (white-noise)
	\item \textbf{Mean} is $E[v(t)]=(c_0 + c_1 + ... + c_n)E[\eta(t)]=0$
	\item \textbf{Covariance function}:
	\begin{equation}
  		\gamma(\tau) =
    		\begin{cases}
      			\lambda^2 * \sum_{i=0}^{n - \tau} c_ic_{i-\tau} & |\tau| \leq n\\
      			0 & otherwise\\
    \end{cases}  
    \end{equation} 

	\item \textbf{Transfer function}: $W(z)=c_0+c_1z^{-1}+...+c_nz^{-n}=\frac{c_0z^n+c_1z^{n-1}+...+c_n}{z^n}$. \\
	All poles are then in the origin of the complex plane, while the zeroes depends on the values of the coefficient. 
\end{itemize}
\pagebreak
It exists also the $MA(\infty)$ representation:
\center $v(t) = c_0\eta(t)+c_1\eta(t-1)+...+c_i\eta(t-i)+... + infinite sum$. 
\\ \raggedright \vspace{0.5em}
In this way the variance in particular becomes an infinite sum as well:
\center $\gamma(0) = (c_0^2+c_1^2+... +c_i^2 + ...) \lambda^2$. 
\\ \raggedright \vspace{0.5em}
We have then to impose the \uline{basic condition}: $\sum_{i=0}^{\infty} c_i^2$ needs to be FINITE, because we need to have $|\gamma(\tau)| \leq \gamma(0)$. Under this condition, $MA(\infty)$ is well defined and a stationary process.
\subsection{Auto Regressive Process (AR)}
\center 
$AR(n) : v(t) = a_1v(t-1)+a_2v(t-2)+...+a_nv(t-n) + \eta(t)$
\\
\vspace{0.5em}
\raggedright
Where $n$ is the order of the process, $\eta(t) \sim WN(0,\lambda^2)$. \\
\begin{itemize}
	\item \textbf{Mean} is computed applying $E[v(t)]$.
	\item \textbf{Covariance function} with $n=1$ (case of $AR(1)$):
	\begin{equation}
  		\gamma(\tau) =
    		\begin{cases}
      			a\gamma(\tau-1) & |\tau| \geq 0\\
      			\frac{1}{1-a^2} \lambda^2 & \tau=0\\
    \end{cases}       
\end{equation}
	well defined if $|a|<1$ (Yule Walker Equations)
	\item \textbf{Transfer function}: $W(z)=\frac{z^n}{z^n-a_1z^{n-1}-...-a_n}$. \\
	\vspace{0.5em}
	Note that poles' position depends on the value $a_1, ..., a_n$. If all poles are inside the unit circle, then the system is \textbf{stable}.
\end{itemize}
\subsection{ARMA Process}
\center 
$ARMA(n_a,n_c) : v(t) = a_1v(t-1)+...+a_{n_a}v(t-n_a) + c_0\eta(t) + ... + c_1\eta(t-1) + ... + c_{n_c}\eta{t-n_c}$
\\
\vspace{1em}
\raggedright
It is composed by an AR and an MA part. The process is stationary if STABLE, meaning that all the poles needs to be inside the unit circle. \\
An ARMA process can be expressed as $v(t)=\frac{C(z)}{A(z)}\eta(t)$ having:
\center
 $C(z)=c_0+c_1z^{-1}+...+c_{n_c}z^{-n_c}$\\ $A(z)=1-a_1z^{-1}-...-a_{n_a}z^{-n_a}$
 \\
$\rightarrow$ $W(z) = \frac{C(z)}{A(z)}$
\\ \raggedright \vspace{0.5em}
Using the \uline{Long Division Algorithm} one can obtain the \textbf{impulse response} representation of it, which is a sort of $MA(\infty)$ expression of the process, where $W(z)=w_0+w_1z^{-1}+w_2z^{-2}+...$ as the result of the long division between $C(z)$ and $A(z)$.
\pagebreak
\section{Spectral Representation}
By the original definition, the spectrum of a stationary process is the \textit{fourier transform} of its covariance function:
\center
 $\Gamma(\omega)=\digamma[\gamma(\tau)]=\sum_{\tau=-\infty}^{\infty} \gamma(\tau)e^{-j \omega \tau}$
 \\
 \vspace{0.5em}
 \raggedright
 When computing the spectrum, remember the Eulero formula:
 $\frac{e^{j \omega \tau}+e^{-j \omega \tau}}{2} = cos(\omega \tau)$
 \\
 Spectrum properties:
 \begin{itemize}
 	\item $\Gamma(\omega)$ is a \textbf{real function} of the real variable $\omega$
 	\item $\Gamma(\omega)$ is \textbf{periodic} with $T=2 \pi$
 	\item $\Gamma(\omega) =\Gamma(-\omega)$ (\textbf{even function}) since both $\gamma(\bullet)$ and $cos(\bullet)$ are even functions
 	\item $\Gamma(\omega) \geq 0$  $\forall \omega$
 \end{itemize}
 Remember that if the spectrum is higher with low frequencies, means that the signal moves very slowly. Vice versa, if the spectrum is higher with high frequencies the signal moves very quickly. 
 \begin{figure}[h!]
 \hfill \includegraphics[width=300pt]{images/spectrum.png}\hspace*{\fill}
  \label{fig:spectrum}
\end{figure} \\
 There exist also the \uline{complex spectrum}:
 \center
 $\Phi (z)=\sum_{\tau=-\infty}^{\infty} \gamma (\tau)z^{-\tau} \rightarrow \Gamma(\omega)=\Phi(z=e^{j \omega})$
 \\
 \raggedright
 \vspace{0.5em}
 One can also anti-transform the spectrum to obtain the covariance function:
  \center
 $\gamma(\tau)=\int_{-\pi}^{+\pi} \Gamma(\omega)e^{j \omega \tau} d\omega$
 \\
 \vspace{0.5em}
 \raggedright
  The spectrum of a white noise is constant and equal to its variance.
 Given  $\eta(t) \sim WN(0,\lambda^2)$
 \center 
$\Gamma_\eta(\omega) = \gamma(0) = Var[\eta(t)] = \lambda^2$
\\
\raggedright
\subsection{Fundamental Theorem of Spectral Analysis}
The spectrum of a process $y$ that takes in input a process $u$ is:
\center 
$\Gamma_{yy}(\omega)=|W(e^{j \omega)}|^2*\Gamma_{uu}(\omega)$
\\
 \begin{figure}[h!]
 \hfill \includegraphics[width=200pt]{images/spectrum-theorem.png}\hspace*{\fill}
  \label{fig:spectrum-theorem}
\end{figure}
\raggedright
\section{Canonical Representation of a Stationary Process}
To solve the multiplicity of ARMA models for a stationary process, in which there are many different ARMA representations for the same process, one can use the canonical representation. \\
From a signal, one can build the spectrum (or equivalently the covariance function). Once we have the spectrum, we can derive the canonical spectral factor to solve the prediction problem. Indeed, given a rational process, there is one and only one ARMA representation which is canonical. \\
\pagebreak
\textbf{Properties of Canonical Representation} (referring to the transfer function $W(z)$):
\begin{itemize}
	\item Numerator and denominator have the same degree
	\item Numerator and denominator are monic (the term with the highest power has coefficient equal to 1)
	\item Numerator and denominator are coprime (no common factors to that can be simplified)
	\item Numerator and denominator are stable polynomials: all poles and zeroes of $W(z)$ are inside the unit disk
\end{itemize}
\section{Prediction Problem}
\subsection{Fake Problem}
In the fake problem we want to find the predictor from the noise. \\
We want to compute $\hat{v}(t+r)$ given the past of $\eta$. We can write: \center 
$v(t+r)=\hat{W(z)}\eta(t+r)=\textcolor{red}{\hat{w}_0\eta(t+r)+\hat{w}_1\eta(t+r-1)+...+\hat{w}_{r-1}\eta(t+1)}+\textcolor{blue}{\hat{w}_r\eta(t)+\hat{w}_{r+1}\eta(t-1)+...}=\textcolor{red}{\alpha(t)}+\textcolor{blue}{\beta(t)}$.
\\ \raggedright \vspace{0.5em}
Since $\eta$ is a WN and we don't know anything of its future, \textcolor{red}{$\alpha(t)$} is FULLY UNPREDICTABLE. While \textcolor{blue}{$\beta(t)$} is predictable, cause it depends on the past of $\eta$. Thus we can write:
\center 
$\hat{v}(t+r|t)=\hat{v}(t|t-r)=\beta(t)$
\\ \raggedright \vspace{0.5em}
To evaluate the performance of our prediction we can use the \textbf{prediction error} and its variance:
\center
$\epsilon(t)=v(t)-\hat{v}(t+r|t)$ \\
$Var[\epsilon(t)]=\lambda^2(\hat{w}_0^2+\hat{w}_1^2+...+\hat{w}_{r-1}^2)$
\\ \raggedright \vspace{0.5em}
\textit{Note that the variance of the error increases with r, meaning that more distant prediction result to be less precise.} \\
Practically $\hat{v}(t+r|t)=\hat{W}_r(z)\eta(t)$, where $\hat{W}_r(z)$ is the result of the r-step LONG DIVISION of the numerator and denominator of $W(z)$ in canonical form.
\subsection{True Problem}
In the true problem we want to find the predictor from data, meaning the past values of $v(\bullet)$. The solution to this is simply:
\center $W_r(z)=\hat{W}(z)^{-1}\hat{W}_r(z)$
\\ \raggedright \vspace{0.5em}
with $\hat{W}_r(z)$ being the transfer function of the predictor, $W(z)$ being the transfer function of the original system in canonical form and $\hat{W}_r(z)$ being the solution to the fake problem. Here $"r"$ are the number of steps of the predictor. \\ 
There is a shortcut that can be used with ARMA process, in general:
\center
	$\hat{v}(t|t-1)=\frac{C(z)-A(z)}{C(z)}v(t)$, with $\hat{W}(z)=\frac{C(z)}{A(z)}$
\\ \raggedright
\subsection{Prediction with eXogenous Variables}
An exogenous variable is another input variable $u(t)$ for the system which differently from the WN is a \uline{deterministic variable}.
\subsubsection{ARX Model}
\center $ARX(n_a,_b): v(t)=a_1v(t-1)+...+a_{n_a}v(t-n_a)+b_1u(t-1)+...+b_{n_b}u(t-n_b)+\eta(t)$ 
\\ \vspace{0.3em}
or in operator form
\\ \vspace{0.3em}
$A(z)v(t)=B(z)u(t-1)+\eta(t)$
\\ \raggedright \vspace{0.5em}
meaning that the transfer function from $u$ to $v$ is $\frac{B(z)}{A(z)}$ and the one from $\eta$ to $v$ is $\frac{1}{A(z)}$.
\subsubsection{ARMAX Model}
Similarly to ARX models, it is defined as $A(z)v(t)=C(z)\eta(t)+B(z)u(t-1).$. \\
It can be represented also with the \uline{Box \& Jenkins model}, in which the WN is considered as a disturb (or noise) and $G(z)$ is the effect of the exogenous variable: 
\center 
$y(t)=G(z)u(t)+W(z)\eta(t)$
\\ \raggedright \vspace{0.5em}
The new predictor formula becomes:
\center 
$\hat{y}(t|t-1)=\frac{C(z)-A(z)}{C(z)}y(t)+\frac{B(z)}{C(z)}u(t-1)$
\\ \raggedright
\section{Prediction Error minimization methods}
\textbf{Identification} consists in estimating a model from data. We can define the prediction error as before: $\epsilon(t)=v(t)-\hat{v}(t+r|t)$. We want to minimize this prediction error, and representing it as a WN (fully unpredictable). \\
Steps:
\begin{enumerate}
	\item \textbf{Data collection}: ${u(1),...,u(n)}$ and ${y(1),...,y(n)}$
	\item \textbf{Choice of the model family}: $M(\theta)$ and corresponding $\epsilon_\theta(t)$, where $\theta$ is a vector of parameters
	\item \textbf{Choice of optimization criterion}: for example we can have
	\center
		$J(\theta)=\frac{1}{N}\sum_{t=1}^{N}\epsilon_\theta(t)^2$ mean squared error \\
		$J(\theta)=\frac{1}{N}\sum_{t=1}^{N}|\epsilon_\theta(t)|$ mean absolute error \\
	\raggedright
	\item \textbf{Optimization}: model parameters are obtained with $\theta=minJ(\theta) \rightarrow \frac{dJ(\theta)}{d\theta} = 0$
	\item \textbf{Validation}: final analysis of the results to evaluate if they satisfy our requirements
\end{enumerate}
\subsection{Least Square method - LS}
Consider the AR or ARX model:
\center $M(\theta):y(t)=a_1(t-1)+...+a_{n_a}y(t-n_a)+b_1u(t-1)+...+u_{n_b}(t-n_b)+\xi(t)= \theta^T\phi(t)+\eta(t)$
\\ \raggedright \vspace{0.5em}
being $\theta = |a_1, ... a_{n_a}, b_1, ..., b_{n_b}|$ and 
$\phi(t)=|y(t-1), ... y(t-n_a), u(t-1), ..., u(t-n_b)|$. \\
Considering as optimization criterion the Mean Squared Error, we impose its derivative equal to zero and we obtain the \textbf{normal equations} and the parameters estimate ($\hat{\theta}$).
\center
	$\sum_{t=1}^N\phi(t)\phi^T(t)\theta = \sum_{t=1}^Ny(t)\phi^T(t)$ $\rightarrow$ $\hat{\theta}=[\sum_{t=1}^N\phi(t)\phi^T(t)]^{-1}*\sum_{t=1}^Ny(t)\phi^T(t)$ 
\\ \raggedright \vspace{0.5em}
Depending on the value of the second derivative, the matrix $\frac{d^2J(\theta)}{d\theta^2}$, we can have two situations:
\begin{enumerate}
	\item \textbf{Positive Definite Matrix}: one point of minimum $\hat{\theta}$ and $J(\theta)$ is a bowl (a paraboloid with vertex in $\hat{\theta}$
	\item \textbf{Positive Semidefinite Matrix (but not positive definite)}: infinite many solutions ($ J(\theta)$ is similar to a section of a pipe)
\end{enumerate}
Let's now consider the $R(N)$ matrix, defined as $R(N)=\frac{1}{N}\sum_{t=1}^N\theta(t)\theta^T(t)$. \\
In an ARX(1,1) we have the following:
\center
$R(N) = \begin{bmatrix}
\frac{1}{N}y(t-1)^2 & \frac{1}{N}y(t-1)u(t-1)\\
\frac{1}{N}u(t-1)y(t-1) & \frac{1}{N}u(t-1)^2
\end{bmatrix}$ \\ \vspace{0.3em}
\textit{Note that the two elements in the main diagonal are respectively the sample variance of $y$ and the sample variance of $u$}. \\
\raggedright
\pagebreak
Bringing $N \rightarrow \infty$:
$\bar{R}= \begin{bmatrix}
\bar{R}_{yy} & \bar{R}_{yu}\\
\bar{R}_{uy} & \bar{R}_{uu}
\end{bmatrix} = \begin{bmatrix}
\gamma_{yy}(0) & \gamma_{yu}(0)\\
\gamma_{uy}(0) & \gamma_{uu}(0)
\end{bmatrix}$ \\ \vspace{0.5em}
In a general ARX($n_a$,$n_b$) model, we obtain that:
\center
 $\bar{R}_{uu}= \begin{bmatrix}
\gamma_{uu}(0) & \gamma_{uu}(1) & \gamma_{uu}(2) & ...\\
\gamma_{uu}(1) & \gamma_{uu}(0) & ... \\
\gamma_{uu}(2) & ..
\end{bmatrix}$
\\ \raggedright \vspace{0.5em}
This matrix is called \textbf{Toeplitz matrix}: inside each diagonal there's the same element; the one inside the main diagonal is the variance. \textit{(Note that for $\bar{R}_{yy}$ is the same but with $y(\bullet)$).} \\
Our aim is to have $\bar{R}_{uu}$ invertible (positive semi-definite) so that the normal equations will have a unique solution.
\subsection{Estimation of mean, covariance and spectrum}
In this section we refer to a generic estimator from the N data ( e.g., the sample mean or the sample covariance) as $\hat{s}_N$. \\
\subsubsection{Mean}
\textbf{Correctness}: \\
$\hat{s}_N$ is a correct estimator if $E[\hat{s}_N]=\bar{s}$. That is, the expected value of the estimator is equal to the probabilistic mean to be estimated. \\
\textbf{Consistency}: \\
$\hat{s}_N$ is a consistent estimator if $Var[\hat{s}_N] \rightarrow 0$ as $N \rightarrow 0$. That is, the estimate error variance tends to zero as the number of measured data tends to infinity.
\subsubsection{Spectrum}
Consider $\hat{\Gamma}$ the spectrum estimator. It is \textbf{asymptotically correct} because: \\ $E[\hat{\Gamma}(\omega)] \rightarrow \Gamma(\omega)$ as $N \rightarrow \infty$ \\ but \textbf{not consistent}: \\ $Var[\hat{\Gamma}(\omega)] \rightarrow \Gamma(\omega)^2$ as $N \rightarrow \infty$. \\ The problem of inconsistency can be tackled using the Bartlett method.
\subsection{Maximum Likelihood method - ML}
This method is based on ARMA or MA or ARMAX models instead of AR and ARX: no more linearity in the parameters and no normal equations. \\
\center $M(\theta): A(z)y(t)=B(z)u(t-1)+C(z)\eta(t)$
\\ \raggedright \vspace{0.3em}
The procedure is still the same to find $\hat{\theta}$ from data is the same: collect data, compute $J(\theta)$ and then minimize it.
The performance index $J(\theta)$ based on the prediction error still can be the mean square error. Differently from the LS method, the function is now non-convex, thus we need an iterative method to solve the minimization problem.
\subsubsection{The Newton Method}
Let's suppose without loss of generality, that $\theta$ is a scalar. The basic idea of this iterative procedure is to \textbf{approximate $J(\theta)$ with a quadratic function $V(\theta)$}. The minimum of this function for the $r^{th}$ iteration will be considered as the estimated vector of parameters $\hat{\theta}^{r+1}$ of the following iteration. \\ By letting $r \rightarrow \infty$ we obtain the minimum $\hat{\bar{\theta}}$. But there is no guarantee that the minimum found is a global minimum. One simple method to deal with this problem is to execute multiple times the algorithm with different initializations and take the best among the different runs. \\
Procedure:
\begin{enumerate}
	\item At iteration $"r"$ we have to estimate $\hat{\theta^r}$
	\item From $\hat{\theta}^r$ we can obtain $A^r(z),B^r(z),C^r(z)$
	\item We can obtain $\epsilon^r(t)$ by means of those: $C^r(z)\epsilon^r(t)=A^r(z)y(t)-B^r(z)u(t-1)$
	\item Filter data to obtain the gradient vector $\Psi^r(t) = - \frac{d\epsilon^r(t)}{d\theta}$
	\item Use Gauss-Newton formula to compute $\hat{\theta}^{(r+1)}$ = $\theta^{(r)}+(\sum_t\Psi(t)\Psi^T(t))^-1\sum_t\Psi(t)\epsilon(t)$
	\item Repeat until convergence
\end{enumerate}
\subsection{Performance of prediction error identification methods}
If we construct the prediction error as usual:
\center
	$\epsilon_\theta(t)=y(t)-\hat{y}_\theta(t)$
\\ \raggedright
Both $y(t)$ and $\hat{y}_\theta(t)$ are sequence of points. Thus the performance index $J(\theta)$ depends on specific points that are provided. To highlight it, we add the subscript N:
\center
	$J_N(\theta) = \frac{1}{N}\sum_{t=1}^N\epsilon_\theta(t)^2$
\\ \raggedright \vspace{0.5em}
Then also the estimated parameters $\hat{\theta_N}$ depends on the data points. \\If the prediction error can be seen as a stationary process,we expect that with $N \rightarrow \infty$:
\center $J_N(\theta) \rightarrow \bar{J}(\theta)=Var[\epsilon_\theta(t)]$\\  $\hat{\theta}_N \rightarrow \bar{\theta}$
\\ \raggedright \vspace{0.3em}
$\bar{J}(\theta)$ does not depend on the particular outcome of the random experiment and so will have a unique minimum.
 \begin{figure}[h!]
 \hfill \includegraphics[width=185pt]{images/pem.png}\hspace*{\fill}
  \label{fig:pem}
  \caption{When $N \rightarrow \infty$ we have a unique deterministic curve}
\end{figure} \\
The performance of a prediction error identification method can be studied asymptotically by analysing the properties of $M(\bar{\theta})$. Having a system $S \in M(\theta)$, we can define $S=M(\theta^\cdot)$ with $\theta^\cdot$ fixed and we can study the rate of convergence of $\hat{\theta_N}$ to $\theta^\cdot$ by performing the variance of their difference.
\section{Model Complexity Selection}
After computing $\hat{\theta}_N$, we have $J_N(\hat{\theta}_N)^{(n)}$ for a model of order $n$. How can we select the best value of $n$, namely the best model complexity?
\begin{itemize}
	\item \textbf{Naive approach}: compute the performance index $J_N$ for multiple increasing values of $n$ until we find the best performance
	\item \textbf{Cross Validation}: divide the dataset with identification and validation set; use the former to build the model and evaluate the performance index using the validation set (we are wasting some of the data, because they canno be used in the identification process)
	\item \textbf{Final Prediction Error}: $FPE=\frac{N+n}{N-n}J_N(\hat{\theta}_N)^{(n)}$ \\
	We are giving a penalty to the models with high complexity. The FPE functions is not monotonically decreasing, and the complexity corresponding to its minimum value can be chosen as complexity of the model
	\item \textbf{Akaike Information Criterion}: $AIC=\frac{2n}{N}+\ln{J_N(\hat{\theta}_N)^{(n)}}$ \\ For high values of N, this is equivalent to FPE. \\ The \uline{first term is the one regarding the complexity of the model}, while \uline{the second is the one regarding the fitting of the data}
	\item \textbf{Minimum Description Length}: $MDL=\frac{n}{N}\ln{N}+\ln{J_N(\hat{\theta}_N)^{(n)}}$ \\
	Asymptotically is simliar to AIC but with lower penalization $\rightarrow$ MDL leads to more parsimonious models
\end{itemize}
 \begin{figure}[h!]
 \hfill \includegraphics[width=300pt]{images/summary-criterion.png}\hspace*{\fill}
  \label{fig:summary-criterion}
\end{figure}
\pagebreak
\section{Durbin-Levinson algorithm}
A recursive algorithm to estimate the parameters of an $AR(n+1)$ starting from an $AR(n)$ model. Using this algorithm we avoid to invert several matrices, which is an expensive procedure. \\
Procedure for $AR(n) \rightarrow AR(n+1)$:
 \begin{figure}[h!]
 \hfill \includegraphics[width=150pt]{images/durbin.png}\hspace*{\fill}
  \label{fig:durbin}
\end{figure} 
\subsection{Time series analysis made easy}
Now we can see easily estimate the order of $MA$ and $AR$ processes.
\subsubsection{MA Models}
We know that $\gamma_y(\tau)=0$ where $\tau > n$. From that we can determine the order of our MA model by finding the value of $\tau$ for which the covariance function goes to zero.
\subsubsection{AR Models} 
In this case the $PARCOV(\tau)$ function may be useful. Considering the two models:
\center 
	$AR(k-1):y(t)=a_1^{(k-1)}y(t-1)+...+a_{k-1}^{(k-1)}y(t-k+1) + \eta(t)$ \\
	$AR(k):y(t)=a_1^{(k)}y(t-1)+...+a_{k}^{(k)}y(t-k) + \eta(t)$ \\
	$PARCOV(\tau)=a_\tau^{(\tau)}$ \\
	\raggedright \vspace{0.5em}
	Namely, the PARtial COVariance function is the last identifier parameter $a_\tau$ of an $AR(\tau)$ mode. One then can estimate $\gamma(\tau)$ and then $PARCOV(\tau)$ to finally decide the order of the system. Indeed, if the order of the "true" AR model is $n$ then:
	\center $PARCOV(\tau)=0$ $\forall \tau > n$
	\\ \raggedright
\subsubsection{More on PARCOV}
In general the partial covariance function can be used to determine if the model to be used should be an AR or an MA:
\begin{itemize}
	\item if, at a certain point, the covariance function goes to zero before the partial covariance does, the process is an MA
	\item if the partial covariance function goes to zero first, the process is an AR
\end{itemize}
\section{Recursive Least Squares - RLS}
All the algorithms seen so far are batch methods that work "\textit{offline}", that use all the data at once. Now we will see a recursive method that is able to update the estimate by adding new data. The latter methods help to overcome the limitation of when the data are coming some at a time, thus this method can work "\textbf{\textit{online}}". \\
$\mu$ is called \textbf{forgetting factor}, and is a parameters that allows to give more importance to the new data with respect to the old one. Indeed it is applied to the original LS formula in this way: 
\center
	$S(t)=\mu S(t-1)+\phi(t)\phi^T(t)$
\\ \raggedright
Note that if $\mu=1$ the method is the normal LS estimation.
\section{Appendix: Example Questions}
\begin{center}
\begin{tabular}{ | m{200pt} | m{200pt}| }
\hline
 The variance matrix is a Toeplitz matrix. & \textbf{False}. The Toeplitz matrix is used to guarantee that an even function $\gamma_y(\tau)$ is indeed a covariance function. Hence, $\gamma_y(\tau)$ is the covariance function of a stationary process if the associated Toeplitz matrix is positive semi-definite for each N. In such case,$\gamma_y(\tau)$ is a \textit{positive semi-definite function}. \\
 \hline
 For a generic random process, its covariance matrix is a Toeplitz matrix? & \textbf{False}. It would be true if we assume the process to be stationary. \\ 
 \hline 
 The covariance coefficient of a random vector of length 2 is:
$\frac{\sqrt{\lambda_{11}}*\sqrt{\lambda_{22}}}{\lambda_{12}}$ & \textbf{False}. $\rho=\frac{\lambda_{12}}{\sqrt{\lambda_{11}}*\sqrt{\lambda_{22}}}$ \\
\hline
The variance matrix is positive definite. & \textbf{False}. It is positive semi-definite.
\\ \hline
A random process depends only on time. & \textbf{False}. A random process is indexed with $t$ but depends also on the single outcome of a random experiment $s$.
\\ \hline
The Covariance concept is different from cross variance. & \textbf{False.} They are the same.
\\ \hline
The whiteness property states that if a process has its real spectrum equal to a $K>0$ in zero and $0$ elsewhere, the it is a WN. & \textbf{False}. The whiteness property refers to the covariance of the process. The spectrum is instead a real and even function of $\omega$, periodic of $2\pi$, with values always greater than or equal to 0.
\\ \hline
The whiteness property states that the covariance function of a $WN \sim (0,\lambda^2)$ is:
\begin{equation*}
  		\gamma(\tau) =
    		\begin{cases}
      			\lambda^2 & \tau = 0 \\
      			0 & otherwise
    \end{cases}  
\end{equation*} & \textbf{True}. The covariance function of a WN process is an impulsive function, centered in zero.
\\ \hline
The real spectrum of a WN is its variance & \textbf{True}. From the definition of spectrum: $\Gamma(\omega)=\sum_{\tau=-\infty}^{\infty} \gamma(\tau)e^{-j \omega \tau}$. The only term remaining from the sum is the one in zero, namely $\gamma(0)=\lambda^2$ (because the others are null).
\\ \hline
Given a spectrum with predominant high frequencies (high values among $\pi$ and $-\pi$), the \textcolor{red}{(a)} graph is the most likely signal from which it is obtained. \\
\begin{minipage}{.4\textwidth}
      \includegraphics[width=\linewidth]{images/spectrum-graph.png}
    \end{minipage}
      & \textbf{True}. Since the high frequencies are predominant, we see that \textcolor{red}{a} has very fast changes of the intensity that's why it is the most likely realization of the process.
      \\ \hline
      Given $\eta(t) \sim WN(0,\lambda^2)$ and a process with the following behavior: \\ $v(t)=c_0\eta(t)+c_1\eta(t-1)$, $c_0,c_1 \in {\rm I\!R}$. \\
      Its complex spectrum is equal to $(c_0^2+c_1^2)\lambda^2$.
      & \textbf{False}. $v(t)$ is an MA(1) process. We know that the variance of such process is $Var[v(t)]=(c_0^2+c_1^2)\lambda^2$. Thus this is not the spectrum.
      \\ \hline
     
\end{tabular}
\end{center}
\begin{center}
\pagebreak
\begin{tabular}{ | m{200pt} | m{200pt}| }
\hline
 An AR(2) process has 2 poles.
      & \textbf{True}. \linebreak $AR(2): v(t)=a_1v(t-1)+a_2v(t-2)+\eta(t)$. Its transfer function is $W(z)=\frac{z^2}{z^2-a_1z-a_2}$. Indeed, its denominator has 2 solutions, that corresponds to the poles. 
	Moreover such an AR(2) will be stable if $|a_2|<1$, since that parameter is the product of the roots of the denominator.
      \\ \hline
	 Given $\eta(t) \sim WN(0,\lambda^2)$ and a process with the following behavior: \\ $v(t)=c_0\eta(t)+c_1\eta(t-1)$, $c_0,c_1 \in {\rm I\!R}$. \\
      Its real spectrum will never have zeroes.
      & \textbf{False.} That process is an MA(1). \linebreak The spectrum of such process is: $\gamma(\omega)=[(c_0^2+c_1^2+2c_0c_1cos(\omega))]\lambda^2$. Thus, it can be null for some values of $c_0$ and $c_1$. For example, when $c_0=c_1$ and $\omega=\pi$, the spectrum is null. 
      \\ \hline 
      Given $\eta(t) \sim WN(0,\lambda^2)$ and a process with the following behavior: \\ $v(t)=c_0\eta(t)+c_1\eta(t-1)$, $c_0,c_1 \in {\rm I\!R}$. \\
      Its complex spectrum is:
      $(c_0^2+c_1^2)\lambda^2 + 2c_0c_1\lambda^2cos(\omega)$
      & \textbf{False}. That is the formula to compute the real spectrum for an MA(1) process.
     \\ \hline 
     An AR(2) process has always 2 zeroes. & \textbf{True}. It has 2 zeroes both in zero.
     \\ \hline
     An $MA(\infty)$ process is the same as an $AR(1)$? & \textbf{False}. Actually, an $AR(n)$ can be reduced to an $MA(\infty)$ applying some constraints on the coefficients ($|a|<1$). 
     \\ \hline
     To obtain Yule Walker Equations from a process $v(t)$, you can sum to both parts of the equation $v(t-\tau)$ and then compute the covariance function. & \textbf{False}. You should multiply, not sum.
     \\ \hline
     An $AR(10)$ process has no zeroes in its transfer function. & \textbf{False}. It has 10 zeroes, all in zero.
     \\ \hline
     An $MA(10)$ process may have no zeroes or poles in its transfer function. & \textbf{False.} It always have both.
     \\ \hline
     An $ARMA(1,1)$ may have no zeroes or poles. & \textbf{True}. If they can be simplified or if it would have been in canonical form it could have been true.
     \\ \hline
     The complex spectrum is the same thing as the real spectrum. & \textbf{False}. The real spectrum is equivalent to the complex spectrum evaluated for $z=e^{j\omega}$.
     \\ \hline
     Given any transfer function with input $u$ and output $y$, is it possible to invert the transfer function? & \textbf{False}. In order to invert it, it should be in canonical form or at least NUM and DEN should have same degree, with poles and zeroes inside unit circle.
     \\ \hline
     Every process has a canonical representation. & \textbf{False}. Processes that are not stochastic and stationary don't have it.
     \\ \hline Every stationary sthocastic process has a canonical representation. & \textbf{True}. In particular, given a rational process there is one and only one ARMA representation of it which is canonical. This special representation is used to solve the problem of multiplicity of ARMA models for which there are many different ARMA models for the same process.
     \\ \hline
     For every possible process, we can write a predictor. & \textbf{True}.
     \\ \hline
     Yule-Walker equations can be applied only to $AR(N)$ processes.
     & \textbf{False}. They can be used on $ARMA$ and $MA$ too, but could be much less useful (especially in $MA$).
     \\ \hline
     Is FPE monotonically decreasing? & \textbf{False}. None of those methods are.
     \\ \hline
     AIC achieves the minimum for lower orders w.r.t. MDL. & \textbf{False}. Is the contrary. MDL achieves the minimum for lower order w.r.t. AIC.
     \\ \hline
     \end{tabular}
\end{center}
\pagebreak
\begin{center}
\begin{tabular}{ | m{200pt} | m{200pt}| }
\hline
Anderson Whiteness Test allows you to conclude that the prediction error is certainly a WN. & \textbf{False}. It only provides a probability about it.
     \\ \hline
     In the end, to see if the estimated model is valid, we need to detect if the prediction error is a WN. & \textbf{True}. We want to reduce our prediction error to a white noise because to improve our estimation, it should be fully unpredictable.
     \\ \hline
     To model systems, we use AR, MA and ARMA model families. & \textbf{False}. We use them for time series analysis.
     \\ \hline
     To model time series, we use ARX and ARMAX. & \textbf{False}. We use them for systems.
     \\ \hline
     Least squares method can be used with ARMA, ARMAX and MA. & \textbf{False}. LS method is only for AR/ARX models.
     \\ \hline
     Let's consider the matrix $R(N)=\frac{1}{N}\sum_{t=1}^N\phi(t)\phi^T(t)$. It always have only one solution. & \textbf{False}. Only if the matrix is positive semi-definite and also invertible, then the normal equations have a unique solution.
     \\ \hline
     An estimator is consistent if its variance is the same of the real WN. 
     & \textbf{False}. An estimator is consistent if the error variance tends to 0 as $N \rightarrow \infty$.
     \\ \hline
     An estimator is correct when its expected value is the same of the white noise. & \textbf{False}. An estimator is correct if its expected value is the probabilistic mean to be estimated.
     \\ \hline
     Maximum Likelihood has non-linear parameters. & \textbf{True}. Instead the Least Squares method is linear in parameters.
    \\ \hline
    ARMA/ARMAX are better to be estimated with Least Squares method. 
    & \textbf{False}. Only AR/ARX can be used with Least Squares.
    \\ \hline
    To estimate the covariance, we apply the Bartlett method. & \textbf{False}. With Bartlett we estimate the spectrum.
    \\ \hline
    The reason why Bartlett method uses large amount of data is because it requires to build many subseries of data. & \textbf{True}.
    \\ \hline
    The Newton method guarantees to find global minimum. & \textbf{False}. There is no guarantee that the minimum found is a global minimum. 
    \\ \hline
    A model of a process is reliable if its error w.r.t. the measured data is a WN. & \textbf{True}. The error should be a fully unpredictable signal.
    \\ \hline
    FPE penalizes model complexity. & \textbf{True}.
    \\ \hline
    AIC penalizes model complexity as much as FPE with N big, while it penalizes more than FPE with small N. & \textbf{True}.
    \\ \hline
    MDL penalizes model complexity more than AIC. & \textbf{True}.
    \\ \hline
    The variance of a correct and consistent estimator is 0. & \textbf{False}. The variance of its error must be zero when $N \rightarrow \infty$.
    \\ \hline
\end{tabular}
\end{center}
\pagebreak
\section{Appendix: Exam 18/06/2020}

\begin{enumerate}
	\item 
	\question{For a Moving Average process, \boldmath{$y(t)=c_0e(t)+c_1e(t-1)+c_2e(t-2)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1.}
	{The value taken by $E[y(t)y(t-1)]$ is $c_0c_1+c_1c_2$}
	{The value taken by $E[y(t)y(t-2)]$ is $c_0c_1+c_1c_2+c_0c_2$}
	{The value taken by $E[y(t)y(t-3)]$ is $0$}
	{The value taken by $E[y(t)y(t-3)]$ is $c0_c2$}
	\item \question{Consider the system with input $u(t)$, output $y(t)$ and transfer function \boldmath{$W(z)=\frac{z+1}{z+0.5}$}. If $u(t)$ is white noise, then the spectrum of the output is:}{Null at $\omega=0$}{Null at $\omega=\pi$}{Null at $\omega=-\pi$}{Null at $\omega=3\pi$}
	\item 
	\question{Consider the process \boldmath{$y(t)=\frac{C(z)}{A(Z)}e(t)$} where \boldmath{$C(z)=1+2z^{-1}, A(z)=1+\frac{1}{2}z^{-1}$} and \boldmath{$e(t)$} is a white noise with zero expected value and variance 1. The process covariance is denoted by $\gamma(\tau)$.}{$\frac{C(z)}{A(z)}$ is in canonical representation}{$\gamma(0)=4$}{$\gamma(1)=0$}{$\gamma(2)=0.25$}
	\item 
	\question{The \textit{Akaike Information Criterion (AIC)} is a method}{To determine the optimal complexity of a stochastic model}{To estimate the parameters of an ARMA model}{To find the optimal predictor of an AR model}{None of the other replies}
	\item \question{Consider the Auto-Regressive process generated as \boldmath{$y(t)=0.7y(t-1)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1.}{The variance of the prediction error of the optimal one step ahead predictor is 1}{The variance of the prediction error of the optimal one step ahead predictor is 0.7}{The variance of the prediction error of the optimal two steps ahead predictor is 1.49}{The variance of the prediction error of the optimal two steps ahead predictor is 0.7}
	\item \question{Consider a stationary process with covariance function \boldmath{$\gamma(\tau)$} and \boldmath{$\Gamma(\omega)$} spectrum. If \boldmath{$\Gamma(\omega)=4,\forall\omega$}, then:}{$\gamma(1)=0)$}{$\gamma(1)=4$}{$\gamma(-1)=4$}{$\gamma(-1)=0$}
	\item \question{The data of a stationary process are used to estimate an Auto-Regressive model of order 1: $\boldmath{y(t)=ay(t-1)+e(t)}$, where \boldmath{$e(t)$} is a white noise. What value can take the estimated parameter a?}{$a=2$}{$a=3$}{$a=5$}{$a=-2$}
	\item \question{Consider a stationary process \boldmath{$v_1(t)$} with covariance function $\gamma_1(\tau)$ and process \boldmath{$v_2(t)=v_1(t-1)$} with covariance function $\gamma_2(\tau)$.}{$\gamma_2(\tau)=\gamma_1(\tau-1), \forall \tau$}{$\gamma_2(\tau)=\gamma_1(\tau+1), \forall \tau$}
	{$\gamma_2(\tau)=\gamma_1(\tau), \forall \tau$}
	{$\gamma_2(\tau)=\gamma_1(\tau+2), \forall \tau$}
	\item 
	\question{The \textit{Yule-Walker} equations}{can be used to estimate the parameters of an Auto-Regressive model}{can be used to estimate the parameters of a Moving-Average model}{can be used to determine the mean value of a stationary process}{can be used to compute the covariance function of an Auto-Regressive model}
	\item \question{Consider the Moving Average process \boldmath{$y(t)=e(t)+0.5e(t-1)$}, where $e(t)$ is a white-noise with  zero expected value and variance 2. The covariance function $\gamma(\tau)$ of \boldmath{$y(t)$} is such that:}{$\gamma(1)=10$}{$\gamma(2)=0.5\gamma(1)$}{$\gamma(2)=0$}{$\gamma(3)=0$}
	\item \question{For the Auto-Regressive process generated as \boldmath{$y(t)=ay(t-1)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1 and parameter \textbf{a} is a real number such that $|a|<1$, which is the variance of the process?}{1}{$\frac{a}{1-a^2}$}{$\frac{a^2}{1-a}$}{$\frac{1}{1-a^2}$}
	\item \question{Consider the process \boldmath{$y(t)=e(t)+0.5e(t-1)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1. The covariance function of the process is such that}{$E[y(t)y(t)]=\frac{1}{2},E[y(t)y(t-1)]=\frac{5}{4}$}{$E[y(t)y(t)]=\frac{5}{4},E[y(t)y(t-1)]=\frac{1}{2}$}{$E[y(t)y(t-2)]=0,E[y(t)y(t-3)]=0$}{$E[y(t)y(t-2)]=\frac{1}{4},E[y(t)y(t-3)]=\frac{1}{8}$}
	\item\question{Consider an \textit{all pass filter} with input \textbf{u} and output \textbf{y}}{If input \textbf{u} is an Auto-Regressive process, output \textbf{y} is a Moving Average process.}{If input \textbf{u} is a constant, output \textbf{y} is a constant.}{If input \textbf{u} is a white noise, output \textbf{y} is a white noise.}{\boldmath{$y(t)=u(t), \forall t$}}
	\item \question{The spectrum \boldmath{$\Gamma(\omega)$} of a stationary process of variance \boldmath{$\lambda^2$}}{is a periodic function of $\omega$ of periord $2\pi$}{is negative for $\omega < 0$}{is such that $\Gamma(-\omega)=\Gamma(\omega)$}{the area behind the spectrum from $\omega=0$ to $\omega=\pi$ is: $\pi * \lambda^2$}
	\pagebreak
	\item \question{Consider the stochastic process \boldmath{$y(t)=-y(t-1)-0.25y(t-2)+e(t)$}, where \boldmath{$e(t)$} is a white noise with expected value 1 and variance 1. Which is the expected value of the process $y(t)$?}{4/9}{0}{2}{1/2}
	\item \question{Given a random variable $v_1$ with zero expected value and variance 2, consider variable \boldmath{$v_2=av_1$} where \textbf{a} is a real and non null parameter.}{	The expected value of $v_2$ is $a$}{The expected value of $v_2$ is 0}{The variance of $v_2$ is $a^2$}{The variance of $v_2$ is $2a^2$}
	\item \question{Consider the Auto Regressive process \boldmath{$y(t)$} generated as \boldmath{$y(t)=0.5y(t-1)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1. The covariance function $\gamma(\tau)$ of $y(t)$ is such that:}{$\gamma(2)=0.5\gamma(1)$}{$\gamma(2)=0$}{$\gamma(3)=0.5\gamma(2)$}{$\gamma(3)=0.5^3\gamma(0)$}
	\item \question{Consider the Auto-Regressive process generated as \boldmath{$y(t)=0.7y(t-1)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1. The optimal one step ahead predictor from data is \boldmath{$\hat{y}(t|t-1)=b_1y(t-1)+b_2y(t-2)$} with:}{\boldmath{$b_1=0, b_2=0$}}{\boldmath{$b_1=0.7, b_2=0$}}{\boldmath{$b_1=0, b_2=0.7$}}{None of the other replies.}
	\item \question{Consider the ARMA process generater as \boldmath{$y(t)=0.5y(t-1)+e(t)+0.5e(t-1)$}, where \boldmath{$e(t)$} is a white noise with expected value $m$ and variance $\lambda^2$. The covariance function $\gamma(\tau)$ of $y(t)$:}{Tends to zero when $\tau$ tends to $\infty$}{Tends to 1 when $\tau$ tends to $\infty$}{Oscillates between $+\lambda^2$ and $-\lambda^2$}{Does not depend upon $m$}
	\item \question{Consider the Auto-Regressive process generated as \boldmath{$y(t)=0.7y(t-1)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1. The variance of the prediction error of the optimal three steps ahead predictor is:}{1.1}{1.25}{1.15}{None of the other replies.}
	\pagebreak
	\item \question{Consider two uncorrelated stationary process \boldmath{$v_1(t)$} and \boldmath{$v_2(t)$} with spectra \boldmath{$\Gamma_1(\omega)$} and $\Gamma_2(\omega)$ respectively. The spectrum \boldmath{$\Gamma(\omega)$} of the process \boldmath{$v(t)=2v_1(t)+3v_2(t)$} is given by:}{$\Gamma(\omega)=2\Gamma_1(\omega)+3\Gamma_2(\omega)$}{$\Gamma(\omega)=4\Gamma_1(\omega)+9\Gamma_2(\omega)$}{$\Gamma(\omega)=\Gamma_1(\omega)*\Gamma_2(\omega)$}{$\Gamma(\omega)=\Gamma_1(\omega)^2*\Gamma_2(\omega)^2$}
	\item \question{Consider the process \boldmath{$y(t)=e(t)+ce(t-1)+bu(t-1)$} where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1.}{This is an ARX process (namely an Auto-Regressive process with eXogenous variable.}{If input $u(t)=1, \forall t$, then process $y(t)$ has zero expected value.}{Given sequences of measurements of $u(t)$ and $y(t)$, parameters $b$ and $c$ can be estimated with the LS (Least Squares) method.}{Given sequences of measurements of $u(t)$ and $y(t)$, parameters $b$ and $c$ can be estimated with the Yule Walker equations.}
	\item \question{The process \boldmath{$y(t)=c_1e(t-1)+c_2e(t-2)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1, is a stationary random process if:}{\boldmath{$c_1=0,c_2=0$}}{\boldmath{$c_1=10,c_2=0.1$}}{\boldmath{$c_1=0.1,c_2=-0.1$}}{\boldmath{$c_1=-10,c_2=1$}}
	\item \question{Consider the process \boldmath{$y(t)=ay(t-1)+e(t)+b_1u(t-1)+b_2u(t-2)$} where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1. Parameter $a$ is such that $a \neq 0$ and $|a|<1$.}{This is an ARX process namely an Auto-Regressive process with eXogenous variable.}{The vector of model parameters is $\theta=a,b_1,b_2$.}{If $u(t)=1$ for all t then $y(t)$ has expected value $b_1+b_2$}{Given sequences of measurements of $u(t)$ and $y(t)$, parameters $a,b_1 and b_2$ can be estimated with the LS (Least Squares) method.}
	\item \question{The system \boldmath{$y(t)=ay(t-1)+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance $\lambda^2$ and \textbf{a} is a real number, generates a stationary process if:}{$a=-2$}{$a=-0.2$}{$a=+0.7$}{$a=2$}
	\item \question{Consider the ARMA process generated as \boldmath{$y(t)=0.5y(t-1)+e(t)+0.5e(t-1)$}, where $e(t)$ is a white noise with zero expected value and variance 1.}{There is a pole in $-0.5$}{There is a zero in $-0.5$}{The spectrum $\Gamma(\omega)$ of $y(t)$ is such that: $\Gamma(\omega)=1, \forall \omega$}
	{The spectrum $\Gamma(\omega)$ of $y(t)$ is such that: $\Gamma(\omega)$ tends to zero when $\omega$ tends to $\infty$}
	\pagebreak
	\item \question{For a stationary process $y(t)$ with $E[y(t)y(t+1)]=1$, what is the value taken by $E[y(t)y(t-1)]$?}{-0.4}{0}{-1}{+1}
	\item \question{The \textit{Maximum Likelihood} method}{can be used to estimate the parameters of an ARMAX model.}{is an iterative estimation method requiring a processing of all available data at each iteration.}{can be used to determine the optimal predictor of an ARMAX model.}{is a method to test the whiteness of a signal.}
	\item \question{Consider a discrete time system with transfer function $W(z)=\rho \frac{z+c}{z+a}$, where $\rho, a, c$ are real parameters. In order to have an all-pass filter it is required that}{$\rho=1$}{$a=-c$}{$a=\frac{1}{c}$}{$a=\rho$}
	\item \question{Conside the Moving-Average process as \boldmath{$y(t)=e(t)+0.5e(t-1)$} where $e(t)$ is a white noise with zero expected value and variance 1. The one step ahead optimal predictor is:}{$\hat{y}(t|t-1)=-0.25\hat{y}(t-1|t-2)+y(t-1)$}{$\hat{y}(t|t-1)=y(t-1)$}{$\hat{y}(t|t-1)=y(t)$}{None of the other replies.}
	\item \question{Conside the Moving-Average process as \boldmath{$y(t)=e(t)+0.5e(t-1)$} where $e(t)$ is a white noise with zero expected value and variance 1}{The two steps ahead optimal predictor is $\hat{y}(t|t-2)=-0.125\hat{y}(t-1|t-2)+y(t-2)$}{The two steps ahead optimal predictor is $\hat{y}(t|t-2)=0$}{The variance of the prediction error of the two steps ahead optimal predictor is $1.25$}{The variance of the prediction error of the two steps ahead optimal predictor is $1$}
	\item \question{Consider a random variable $v_1$ with zero expected value and variance 2. A second variable $v_2$ is given by $v_2=av1$ where \textbf{a} is a real non null parameter. The covariance coefficient of the two variables $v_1$ and $v_2$ is:}{0}{1}{a}{independent of the value of the non null parameter a}
	\item \question{The \textit{periodogram}}{provides an approximate predictor of a stationary process}{provides an approximate estimate of the spectrum of a stationary process}{is an iterative procedure to find the covariance function of a stationary process}{is obtained by means of the Fourier transform formula}
	\pagebreak
	\item \question{Consider the process generated as \boldmath{$y(t)=1+e(t)$}, where \boldmath{$e(t)$} is a white noise with zero expected value and variance 1.}{The one step ahead optimal predictor $\hat{y}(t|t-1)$ is equal to 0.}{The one step ahead optimal predictor $\hat{y}(t|t-1)$ is equal to 1.}{The two steps ahead optimal predictor $\hat{y}(t|t-2)$ is equal to 2.}{The two steps ahead optimal predictor $\hat{y}(t|t-2)$ is equal to 1.}
	\item \question{Consider the stochastic process as \boldmath{$y(t)=e(t)+0.5e(t-1)-0.5e(t-2)$} where $e(t)$ is a white noise with zero expected value 1 and variance 1.}{The expected value of the process is 0.}{The expected value of the process is 1.}{The variance of the process is 3/2.}{The variance of the process is 5/4.}
	\item \question{For a given time series, it turns out that the estimated Partial Covariance function $PARCOV(\tau)$ becomes null for $\tau>5$. Which is the appropriate model to fit data?}{$MA(5)$}{$MA(4)$}{$AR(5)$}{$ARMA(3,3)$}
\end{enumerate}
\pagebreak
\subsection{Solutions}
\begin{enumerate}[label=\textbf{\arabic*})]
\item T F T F
\item F T T T
\item F T T F
\item T F F F
\item T F T F
\item T F F T
\item F F F F
\item F F T F
\item T F F T
\item F F T T
\item F F F T
\item F T T F
\item F T T F
\item T F T F
\item T F F F
\item F T F T
\item T F T T
\item F T F F
\item T F F T
\item F F F T
\item F T F F
\item F F F F
\item T T T T
\item T T F T
\item F T T F
\item F T F F
\item F F F T
\item T T F F
\item F F T F
\item F F F F
\item F T T F
\item F T F T
\item F T F T
\item F T F T
\item F T T F
\item F F T F
\end{enumerate}
\end{document}
